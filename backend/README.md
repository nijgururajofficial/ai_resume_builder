# AI Resume Builder with Google Gemini & LangGraph 🚀

[![Python Version](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Project Status](https://img.shields.io/badge/status-active-brightgreen.svg)]()

An intelligent system that generates highly tailored, ATS-compliant resumes by leveraging the power of **Google's Gemini Pro**, orchestrated by **LangGraph**. This tool transforms a generic user profile into a professional resume customized for a specific job description, significantly increasing the chances of passing through automated screening systems.

***

## Table of Contents

* [How It Works: The Agentic Workflow](#how-it-works-the-agentic-workflow)
* [Core Features](#core-features)
* [Response Logging & Monitoring](#response-logging--monitoring)
* [Technology Stack](#technology-stack)
* [Project Structure](#project-structure)
* [Setup and Installation](#setup-and-installation)
* [Usage](#usage)
* [Running Tests](#running-tests)

---

## How It Works: The Agentic Workflow

The application follows a modular, agent-based workflow managed by LangGraph. This ensures a clear separation of concerns and a predictable, stateful process. The system receives a user profile and a job description, then processes them through a sequence of specialized agents to produce the final resume.

1.  **🔍 `JobProspectorAgent`**: This agent can scrape job postings directly from URLs and extract structured information including job title, company name, required skills, and responsibilities. It's perfect for analyzing job postings from various job boards and company websites.

2.  **🤖 `JobDescriptionAnalysisAgent`**: This agent receives the raw job description (either from text or from the JobProspectorAgent) and uses the Gemini API to perform a detailed analysis, extracting the precise **job title**, required **skills** (including important acronyms), and key **responsibilities**.

3.  **📊 `ResumeAnalysisAgent`**: This agent analyzes existing resume text and converts it into structured JSON format, identifying key sections like contact information, work experience, education, and skills. It's useful for parsing existing resumes into the system's format.

4.  **✍️ `ResumeContentSelectionAgent`**: Using the insights from the job analysis agents, this agent selects the most relevant experiences and projects from the user's master profile. It then prompts the Gemini API to rewrite this content, strategically weaving in keywords and quantifying achievements to align perfectly with the job requirements.

5.  **📝 `MarkdownFormattingAgent`**: This is a deterministic agent that takes the tailored content generated by the previous agent and assembles it into a clean, well-structured, and ATS-compliant Markdown format.

6.  **📄 `PdfDocxGenerator`**: Finally, this utility converts the formatted Markdown into two professional outputs: a `.docx` file for easy editing and a visually styled `.pdf` file, ensuring perfect fonts, spacing, and layout for submission.

7.  **📊 `ResponseLogger`**: (Optional) Captures detailed execution data from all agents for debugging, performance monitoring, and continuous improvement.

---

## Core Features

* **✨ AI-Powered Tailoring**: Uses the Google Gemini API to analyze job descriptions and strategically rewrite resume content to match required skills and responsibilities.
* **🔍 Smart Job Scraping**: The JobProspectorAgent can automatically extract job information from URLs, making it easy to analyze job postings from any source.
* **📊 Resume Parsing**: The ResumeAnalysisAgent can parse existing resumes and convert them into structured data for further processing.
* **✅ ATS Optimization**: Strictly adheres to Universal ATS Resume Requirements, ensuring maximum compatibility with systems like Oracle Taleo, Workday, and iCIMS.
* **🧠 Agentic Workflow**: Employs a multi-agent architecture with LangGraph for a robust, debuggable, and transparent pipeline (Scrape/Analyze → Parse → Select → Format → Generate).
* **📊 Quantified Achievements**: Intelligently highlights and prioritizes measurable results and metrics from the user's profile to demonstrate impact.
* **📄 Dual Format Output**: Generates resumes in both `.docx` and visually styled `.pdf` formats, ready for any application platform.
* **🔒 Secure & Configurable**: Manages API keys via environment variables and user data via a simple JSON profile.
* **📈 Performance Monitoring**: Comprehensive logging and analysis of agent execution for debugging and optimization.

---

## Response Logging & Monitoring

The AI Resume Builder includes comprehensive response logging that captures detailed information about each agent's execution. This is invaluable for debugging, performance monitoring, and improving the system.

### What Gets Logged

For each agent execution, the system captures:

- **Input Data**: What was passed to the agent
- **Output Data**: What the agent returned
- **Execution Time**: How long the agent took to complete
- **Status**: Success or error status
- **Error Messages**: Detailed error information if something fails
- **Raw LLM Responses**: For LLM-based agents, the complete response from Gemini
- **Prompts Used**: The exact prompts sent to the LLM
- **Timestamps**: When each agent executed

### Enabling Response Logging

To enable response logging, use the `--log-responses` flag:

```bash
python main.py --job-desc data/job_description.txt --profile data/user_profile.json --log-responses
```

### Generated Files

When response logging is enabled, the system creates:

1. **Agent Response Files** (`output/agent_responses/`):
   - Individual JSON files for each workflow run
   - Complete workflow state including all agent responses
   - Timestamped filenames for easy tracking

2. **Performance Reports** (`output/reports/`):
   - HTML reports with execution statistics
   - Agent performance comparisons
   - Error summaries and debugging information

### Using the Response Logger Programmatically

You can also use the `ResponseLogger` class in your own scripts:

```python
from core.response_logger import ResponseLogger

# Initialize the logger
logger = ResponseLogger(output_dir="output")

# Save workflow responses
response_file = logger.save_workflow_responses(final_state)

# Generate performance report
report_file = logger.generate_performance_report(final_state)

# Analyze agent responses
analysis = logger.analyze_agent_responses(final_state)

# Access response history
history = logger.get_agent_response_history(limit=10)
```

---

## Technology Stack

* **Backend**: Python 3.9+
* **AI/LLM**: Google Gemini Pro
* **Agent Framework**: LangGraph
* **Document Generation**: `python-docx`, `xhtml2pdf`, `WeasyPrint`
* **Data Analysis**: `pandas` for response analysis and reporting
* **Dependencies**: `google-generativeai`, `langchain`, `markdown2`

---

## Project Structure

```
ai_resume_builder/
├── agents/                     # Contains the specialized AI agents
│   ├── __init__.py
│   ├── job_prospector_agent.py      # Scrapes and analyzes job postings from URLs
│   ├── job_description_analysis_agent.py
│   ├── resume_analysis_agent.py     # Parses existing resumes into structured data
│   ├── resume_content_selection_agent.py
│   └── markdown_formatting_agent.py
├── core/                       # Core services and utilities
│   ├── __init__.py
│   ├── gemini_client.py       # Google Gemini API client
│   ├── langgraph_orchestrator.py  # Workflow orchestration
│   ├── pdf_docx_generator.py  # Document generation
│   ├── response_logger.py     # Response logging and analysis
│   └── ats_markdown_template.md
├── data/                       # Input data directory
│   ├── user_profile.json      # User's master profile
│   └── job_description.txt    # Target job description
├── output/                     # Generated outputs
│   ├── agent_responses/       # Agent response logs (when enabled)
│   └── reports/               # Performance reports (when enabled)
├── temp/                       # Temporary intermediate files
├── tests/                      # Unit tests
│   └── test_agents.py
├── .env                        # Environment variables (API keys)
├── main.py                     # Main application entrypoint
├── requirements.txt            # Project dependencies
└── README.md                   # This file
```

---

## Setup and Installation

Follow these steps to set up and run the project locally.

### 1. Clone the Repository

```sh
git clone https://github.com/nijgururajofficial/ai_resume_builder.git
cd ai-resume-builder
```

### 2. Create and Activate a Virtual Environment

* **macOS / Linux**:
    ```sh
    python3 -m venv venv
    source venv/bin/activate
    ```
* **Windows**:
    ```powershell
    python -m venv venv
    .\venv\Scripts\Activate.ps1
    ```

### 3. Install Dependencies

Install all the required Python libraries using `pip`.

```sh
pip install -r requirements.txt
```

### 4. Set Up Environment Variables

Create a file named `.env` in the project's root directory. Get your API key from **[Google AI Studio](https://aistudio.google.com/)** and add it to this file.

```env
# .env
GEMINI_API_KEY="YOUR_GOOGLE_API_KEY_HERE"
```

### 5. Configure Your Profile

Edit the `data/user_profile.json` file to contain your personal information, work experience, education, skills, and projects. Follow the existing JSON structure carefully.

**Important**: Keep both `data/user_profile.json` and `data/job_description.txt` in the `data/` directory. The system expects these files to be in their default locations for optimal functionality.

---

## Usage

The application is run from the command line. You must provide a path to a plain text file containing the job description you are targeting.

### Basic Usage

1.  Place your job description in `data/job_description.txt`.
2.  Ensure your profile is in `data/user_profile.json`.
3.  Run the `main.py` script from the root directory:

    ```sh
    python main.py
    ```

The script will execute the full pipeline and save the generated `.docx` and `.pdf` resumes into the `output/` directory.

### Advanced Agent Capabilities

The system includes several specialized agents that can enhance your resume building workflow:

* **JobProspectorAgent**: Automatically scrapes job postings from URLs and extracts structured information. Perfect for analyzing job postings from LinkedIn, Indeed, company career pages, and other job boards.

* **ResumeAnalysisAgent**: Parses existing resume text (PDF, DOCX, or plain text) and converts it into structured JSON format. Useful for importing your current resume into the system for further customization.

* **JobDescriptionAnalysisAgent**: Analyzes job descriptions to extract key requirements, skills, and responsibilities for optimal ATS matching.

These agents work together to provide a comprehensive resume building experience, from job analysis to final document generation.

### With Response Logging

To enable comprehensive logging and monitoring:

```sh
python main.py --log-responses
```

This will generate additional files for debugging and performance analysis.

### Command-Line Arguments

* `--profile`: (Optional) Path to your user profile JSON file. Defaults to `data/user_profile.json`.
* `--output-dir`: (Optional) Directory to save the generated resumes. Defaults to `output/`.
* `--log-responses`: (Optional) Enable detailed response logging for all agents.

**Note**: The system automatically uses `data/job_description.txt` for the job description, so no `--job-desc` argument is needed.

---

## Running Tests

To ensure all components are working correctly, you can run the built-in unit tests. The tests use mock objects to validate the agents' logic without making actual API calls.

```sh
python -m unittest discover tests
```



---

## Troubleshooting

### Common Issues

1. **PDF Generation Errors**: If you encounter PDF generation issues, the system will automatically fall back to WeasyPrint if available.

2. **API Key Issues**: Ensure your `GEMINI_API_KEY` is properly set in the `.env` file or as an environment variable.

3. **Response Logging**: If response logging fails, check that the `output/` directory is writable and that `pandas` is installed.

### Performance Optimization

- Use response logging to identify slow agents and optimize prompts
- Monitor execution times to identify bottlenecks
- Review generated reports for insights into system performance

---

## Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues for bugs and feature requests.

### Development Guidelines

1. Follow the existing code structure and patterns
2. Add tests for new functionality
3. Update documentation for any API changes
4. Ensure response logging compatibility for new agents

---

## License

This project is licensed under the MIT License - see the LICENSE file for details.
